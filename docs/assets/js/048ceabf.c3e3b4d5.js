"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[4305],{3905:(e,a,t)=>{t.d(a,{Zo:()=>c,kt:()=>f});var i=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function s(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);a&&(i=i.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?s(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,i,n=function(e,a){if(null==e)return{};var t,i,n={},s=Object.keys(e);for(i=0;i<s.length;i++)t=s[i],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(i=0;i<s.length;i++)t=s[i],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var o=i.createContext({}),u=function(e){var a=i.useContext(o),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},c=function(e){var a=u(e.components);return i.createElement(o.Provider,{value:a},e.children)},h="mdxType",p={inlineCode:"code",wrapper:function(e){var a=e.children;return i.createElement(i.Fragment,{},a)}},d=i.forwardRef((function(e,a){var t=e.components,n=e.mdxType,s=e.originalType,o=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),h=u(t),d=n,f=h["".concat(o,".").concat(d)]||h[d]||p[d]||s;return t?i.createElement(f,r(r({ref:a},c),{},{components:t})):i.createElement(f,r({ref:a},c))}));function f(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var s=t.length,r=new Array(s);r[0]=d;var l={};for(var o in a)hasOwnProperty.call(a,o)&&(l[o]=a[o]);l.originalType=e,l[h]="string"==typeof e?e:n,r[1]=l;for(var u=2;u<s;u++)r[u]=t[u];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}d.displayName="MDXCreateElement"},818:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>o,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>l,toc:()=>u});var i=t(7462),n=(t(7294),t(3905));const s={slug:"gaussian",title:"What is Gaussian?",authors:"aneesh",tags:["algo","gaussian","gaussian naive bayes"]},r=void 0,l={permalink:"/CyberSec-NGIT/blog/gaussian",editUrl:"https://github.com/stealthspectre/CyberSec-NGIT/blog/gaussian.md",source:"@site/blog/gaussian.md",title:"What is Gaussian?",description:"What's a Gaussian Distribution?",date:"2023-03-29T11:34:14.216Z",formattedDate:"March 29, 2023",tags:[{label:"algo",permalink:"/CyberSec-NGIT/blog/tags/algo"},{label:"gaussian",permalink:"/CyberSec-NGIT/blog/tags/gaussian"},{label:"gaussian naive bayes",permalink:"/CyberSec-NGIT/blog/tags/gaussian-naive-bayes"}],readingTime:2.125,hasTruncateMarker:!1,authors:[{name:"Aneesh Sambu",title:"stealthspectre",url:"https://github.com/stealthspectre",imageURL:"https://github.com/stealthspectre.png",key:"aneesh"}],frontMatter:{slug:"gaussian",title:"What is Gaussian?",authors:"aneesh",tags:["algo","gaussian","gaussian naive bayes"]},prevItem:{title:"Neural Networks \ud83e\udde0",permalink:"/CyberSec-NGIT/blog/neural-networks"},nextItem:{title:"What are Regressive tasks?",permalink:"/CyberSec-NGIT/blog/regression"}},o={authorsImageUrls:[void 0]},u=[{value:"What&#39;s a Gaussian Distribution?",id:"whats-a-gaussian-distribution",level:2},{value:"What is a Gaussian Naive Bayes Classifier?",id:"what-is-a-gaussian-naive-bayes-classifier",level:2},{value:"Simple Definition",id:"simple-definition",level:3},{value:"Lets dive Deeper",id:"lets-dive-deeper",level:3}],c={toc:u},h="wrapper";function p(e){let{components:a,...t}=e;return(0,n.kt)(h,(0,i.Z)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h2",{id:"whats-a-gaussian-distribution"},"What's a Gaussian Distribution?"),(0,n.kt)("p",null,(0,n.kt)("img",{parentName:"p",src:"https://th.bing.com/th/id/OIP.QSPgbSo7zQz6-jZQUTEwGQHaDh?pid=ImgDet&rs=1",alt:null})),(0,n.kt)("p",null,"The Gaussian distribution is often used to model real-world phenomena that are naturally distributed around a central value, such as the heights of people in a population, the weights of objects, or the errors in a measurement. It is also used in many machine learning algorithms, such as linear regression, logistic regression, and Gaussian mixture models.\nThe Gaussian distribution has several important properties, such as the 68-95-99.7 rule, which states that approximately 68% of the data falls within one standard deviation of the mean, 95% falls within two standard deviations, and 99.7% falls within three standard deviations. This makes it a useful tool for analyzing and modeling data in many different fields."),(0,n.kt)("hr",null),(0,n.kt)("h2",{id:"what-is-a-gaussian-naive-bayes-classifier"},"What is a Gaussian Naive Bayes Classifier?"),(0,n.kt)("h3",{id:"simple-definition"},"Simple Definition"),(0,n.kt)("p",null,"Gaussian naive Bayes classifiers are a type of probabilistic classifier that assume that the features of a data point are independent and normally distributed, and use Bayes' theorem to calculate the probability of each class given the features."),(0,n.kt)("h3",{id:"lets-dive-deeper"},"Lets dive Deeper"),(0,n.kt)("p",null,"Gaussian ",(0,n.kt)("a",{parentName:"p",href:"/docs/ML-Based%20Zero%20Day%20Detection/ml-algorithms#na%C3%AFve-bayes"},"Naive Bayes"),' (GNB) is a probabilistic classifier that is based on Bayes\' theorem and assumes that the features of a data point are independent and normally distributed. It is called "naive" because it makes a strong assumption of independence between the features, which may not always be true in practice. Despite this simplifying assumption, GNB can be surprisingly effective in many real-world applications.'),(0,n.kt)("p",null,"The GNB classifier works by first estimating the mean and variance of each feature for each class in the training data. Then, given a new data point with features x, it calculates the probability of each class c using Bayes' theorem:"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"P(c|x) = P(x|c) ","*"," P(c) / P(x)")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"where P(c|x) is the probability of class c given the features x"),(0,n.kt)("li",{parentName:"ul"},"P(x|c) is the probability of observing the features x given class c (which is modeled as a Gaussian distribution with mean and variance estimated from the training data)"),(0,n.kt)("li",{parentName:"ul"},"P(c) is the prior probability of class c (which is estimated from the relative frequency of each class in the training data)"),(0,n.kt)("li",{parentName:"ul"},"P(x) is the marginal probability of observing the features x (which can be calculated as the sum of P(x|c) ","_"," P(c) over all classes).")),(0,n.kt)("p",null,"Finally, the GNB classifier predicts the class with the highest probability. In practice, GNB can be very fast and efficient, especially for high-dimensional data with many features. However, it may not perform well if the independence assumption is strongly violated or if the data is not well-modeled by a Gaussian distribution."))}p.isMDXComponent=!0}}]);