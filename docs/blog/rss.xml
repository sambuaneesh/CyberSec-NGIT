<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Greetings, Cyber Warrior!üßë‚Äçüíª Blog</title>
        <link>https://stealthspectre.github.io/CyberSec-NGIT/blog</link>
        <description>Greetings, Cyber Warrior!üßë‚Äçüíª Blog</description>
        <lastBuildDate>Wed, 29 Mar 2023 11:34:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[What is Gaussian?]]></title>
            <link>https://stealthspectre.github.io/CyberSec-NGIT/blog/gaussian</link>
            <guid>https://stealthspectre.github.io/CyberSec-NGIT/blog/gaussian</guid>
            <pubDate>Wed, 29 Mar 2023 11:34:14 GMT</pubDate>
            <description><![CDATA[What's a Gaussian Distribution?]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-a-gaussian-distribution">What's a Gaussian Distribution?<a href="#whats-a-gaussian-distribution" class="hash-link" aria-label="Direct link to What's a Gaussian Distribution?" title="Direct link to What's a Gaussian Distribution?">‚Äã</a></h2><p><img loading="lazy" src="https://th.bing.com/th/id/OIP.QSPgbSo7zQz6-jZQUTEwGQHaDh?pid=ImgDet&amp;rs=1" class="img_ev3q"></p><p>The Gaussian distribution is often used to model real-world phenomena that are naturally distributed around a central value, such as the heights of people in a population, the weights of objects, or the errors in a measurement. It is also used in many machine learning algorithms, such as linear regression, logistic regression, and Gaussian mixture models.
The Gaussian distribution has several important properties, such as the 68-95-99.7 rule, which states that approximately 68% of the data falls within one standard deviation of the mean, 95% falls within two standard deviations, and 99.7% falls within three standard deviations. This makes it a useful tool for analyzing and modeling data in many different fields.</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-gaussian-naive-bayes-classifier">What is a Gaussian Naive Bayes Classifier?<a href="#what-is-a-gaussian-naive-bayes-classifier" class="hash-link" aria-label="Direct link to What is a Gaussian Naive Bayes Classifier?" title="Direct link to What is a Gaussian Naive Bayes Classifier?">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="simple-definition">Simple Definition<a href="#simple-definition" class="hash-link" aria-label="Direct link to Simple Definition" title="Direct link to Simple Definition">‚Äã</a></h3><p>Gaussian naive Bayes classifiers are a type of probabilistic classifier that assume that the features of a data point are independent and normally distributed, and use Bayes' theorem to calculate the probability of each class given the features.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lets-dive-deeper">Lets dive Deeper<a href="#lets-dive-deeper" class="hash-link" aria-label="Direct link to Lets dive Deeper" title="Direct link to Lets dive Deeper">‚Äã</a></h3><p>Gaussian <a href="/CyberSec-NGIT/docs/ML-Based Zero Day Detection/ml-algorithms#na%C3%AFve-bayes">Naive Bayes</a> (GNB) is a probabilistic classifier that is based on Bayes' theorem and assumes that the features of a data point are independent and normally distributed. It is called "naive" because it makes a strong assumption of independence between the features, which may not always be true in practice. Despite this simplifying assumption, GNB can be surprisingly effective in many real-world applications.</p><p>The GNB classifier works by first estimating the mean and variance of each feature for each class in the training data. Then, given a new data point with features x, it calculates the probability of each class c using Bayes' theorem:</p><p><strong>P(c|x) = P(x|c) <!-- -->*<!-- --> P(c) / P(x)</strong></p><ul><li>where P(c|x) is the probability of class c given the features x</li><li>P(x|c) is the probability of observing the features x given class c (which is modeled as a Gaussian distribution with mean and variance estimated from the training data)</li><li>P(c) is the prior probability of class c (which is estimated from the relative frequency of each class in the training data)</li><li>P(x) is the marginal probability of observing the features x (which can be calculated as the sum of P(x|c) <!-- -->_<!-- --> P(c) over all classes).</li></ul><p>Finally, the GNB classifier predicts the class with the highest probability. In practice, GNB can be very fast and efficient, especially for high-dimensional data with many features. However, it may not perform well if the independence assumption is strongly violated or if the data is not well-modeled by a Gaussian distribution.</p>]]></content:encoded>
            <category>algo</category>
            <category>gaussian</category>
            <category>gaussian naive bayes</category>
        </item>
        <item>
            <title><![CDATA[What are Regressive tasks?]]></title>
            <link>https://stealthspectre.github.io/CyberSec-NGIT/blog/regression</link>
            <guid>https://stealthspectre.github.io/CyberSec-NGIT/blog/regression</guid>
            <pubDate>Wed, 29 Mar 2023 10:54:53 GMT</pubDate>
            <description><![CDATA[So what's this Regression?]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="so-whats-this-regression">So what's this Regression?<a href="#so-whats-this-regression" class="hash-link" aria-label="Direct link to So what's this Regression?" title="Direct link to So what's this Regression?">‚Äã</a></h2><p>In machine learning, a regression task is a type of supervised learning problem where the goal is to predict a continuous numerical value, such as a price, a temperature, or a stock price. The objective of a regression model is to learn a function that maps input features to a continuous output value.</p><p>Regression tasks are different from classification tasks, where the goal is to predict a categorical label, such as whether an email is spam or not. In a regression task, the output variable is a continuous value, whereas in a classification task, the output variable is a discrete value.</p><p>Regression models can be used for a wide range of applications, such as predicting housing prices based on features like location, square footage, and number of bedrooms, or predicting the temperature based on weather conditions like humidity, wind speed, and cloud cover.</p><p>There are many different types of regression models, including linear regression, polynomial regression, and decision tree regression. The choice of model depends on the specific problem and the characteristics of the data. The performance of a regression model is typically evaluated using metrics like mean squared error (MSE) or root mean squared error (RMSE), which measure the difference between the predicted values and the actual values.</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-main-difference-between-classification-and-regression">What is the main difference between classification and regression?<a href="#what-is-the-main-difference-between-classification-and-regression" class="hash-link" aria-label="Direct link to What is the main difference between classification and regression?" title="Direct link to What is the main difference between classification and regression?">‚Äã</a></h2><p>The main difference between regression and classification tasks in machine learning is the type of output that the model is trying to predict.</p><p>In a regression task, the goal is to predict a continuous numerical value, such as a price, a temperature, or a stock price. The objective of a regression model is to learn a function that maps input features to a continuous output value.</p><p>In a classification task, on the other hand, the goal is to predict a categorical label, such as whether an email is spam or not, or whether a patient has a certain disease or not. The output variable is a discrete value, and the model is trained to classify input data into one of several predefined categories.</p><p>Another key difference between regression and classification tasks is the type of algorithms that are used. Regression models typically use algorithms like linear regression, polynomial regression, or decision tree regression, while classification models use algorithms like logistic regression, decision trees, or support vector machines.</p><p>The evaluation metrics used for regression and classification tasks are also different. For regression tasks, metrics like mean squared error (MSE) or root mean squared error (RMSE) are commonly used to measure the difference between the predicted values and the actual values. For classification tasks, metrics like accuracy, precision, recall, and F1 score are used to measure the performance of the model in correctly classifying the input data.</p>]]></content:encoded>
            <category>regression</category>
            <category>classification</category>
            <category>ml</category>
            <category>algo</category>
        </item>
        <item>
            <title><![CDATA[Diving into KNN]]></title>
            <link>https://stealthspectre.github.io/CyberSec-NGIT/blog/knn</link>
            <guid>https://stealthspectre.github.io/CyberSec-NGIT/blog/knn</guid>
            <pubDate>Wed, 29 Mar 2023 10:45:19 GMT</pubDate>
            <description><![CDATA[Plain Definition]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="plain-definition">Plain Definition<a href="#plain-definition" class="hash-link" aria-label="Direct link to Plain Definition" title="Direct link to Plain Definition">‚Äã</a></h2><p>KNN (K-Nearest Neighbors) is a machine learning algorithm used for classification and <a href="/CyberSec-NGIT/blog/regression">regression tasks</a>. It is a <a href="/CyberSec-NGIT/blog/non-parametric">non-parametric algorithm</a>, which means that it does not make any assumptions about the underlying distribution of the data.</p><p>In the KNN algorithm, the input data is represented as points in a high-dimensional space, and the algorithm classifies new data points based on their proximity to the existing data points. Specifically, the algorithm calculates the distance between the new data point and each of the existing data points, and then assigns the new data point to the class that is most common among its K nearest neighbors.</p><p>The value of K is a hyperparameter that can be tuned to optimize the performance of the algorithm. A larger value of K will result in a smoother decision boundary, but may also lead to misclassification of data points that are close to the boundary between two classes.</p><p>KNN is a simple and effective algorithm that can be used for a wide range of classification and <a href="/CyberSec-NGIT/blog/regression">regression tasks</a>. However, it can be computationally expensive for large datasets, and may not perform well in high-dimensional spaces.</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="real-time-example">Real time example<a href="#real-time-example" class="hash-link" aria-label="Direct link to Real time example" title="Direct link to Real time example">‚Äã</a></h2><p>Imagine you are a penguin living in Antarctica, and you want to find a new place to build your igloo. You have heard that some areas are better than others, but you're not sure which ones. So, you decide to ask your penguin friends for advice.</p><p>You ask your friends to rate different areas on a scale of 1 to 10, based on how good they are for building an igloo. You also ask them to tell you the distance of each area from your current location.</p><p>Now, you have a dataset of ratings and distances for different areas. You want to use this data to find the best place to build your igloo.</p><p>This is where KNN comes in. It can help you find the best place to build your igloo based on the ratings and distances provided by your friends.</p><p>Here's how it works:</p><ol><li>You choose a value for K. This is the number of neighbors you want to consider when making a decision. Let's say you choose K=3.</li><li>You calculate the distance between each area and your current location.</li><li>You find the 3 areas that are closest to your current location.</li><li>You look at the ratings for these 3 areas, and take the average. This gives you a predicted rating for each of the 3 areas.</li><li>You choose the area with the highest predicted rating as the best place to build your igloo.</li></ol><p>So, in this example, KNN helped you find the best place to build your igloo based on the ratings and distances provided by your friends.</p><p>Of course, in real life, KNN can be used for many other things besides finding the best place to build an igloo. For example, it can be used to predict the price of a house based on its features, or to classify images based on their content.</p><p>But hopefully this fun example helps you understand the basic idea behind KNN!</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="now-what-is-this-weighted-knn">Now what is this weighted KNN?<a href="#now-what-is-this-weighted-knn" class="hash-link" aria-label="Direct link to Now what is this weighted KNN?" title="Direct link to Now what is this weighted KNN?">‚Äã</a></h2><p>Weighted KNN is a variation of the KNN algorithm where the contribution of each of the K nearest neighbors is weighted according to their distance from the query point. In other words, the closer a neighbor is to the query point, the more weight it is given in the final prediction.</p><p>In the standard KNN algorithm, all K neighbors are given equal weight in the final prediction. However, this may not always be the best approach, as some neighbors may be more relevant than others depending on their distance from the query point.</p><p>For example, let's say you are trying to predict the price of a house based on its features, such as the number of bedrooms, bathrooms, and square footage. In a standard KNN algorithm, the K nearest neighbors are chosen based solely on their feature values, without considering their distance from the query point. However, it's possible that some of these neighbors are located far away from the query point, and therefore may not be as relevant to the prediction.</p><p>In a weighted KNN algorithm, the contribution of each neighbor is weighted based on its distance from the query point. This means that neighbors that are closer to the query point are given more weight in the final prediction, while neighbors that are farther away are given less weight.</p><p>Using the same example of predicting house prices, this means that the K nearest neighbors are chosen based on both their feature values and their distance from the query point. The closer a neighbor is to the query point, the more weight it is given in the final prediction, as it is considered to be more relevant to the prediction.</p><p>Overall, weighted KNN can be a useful variation of the KNN algorithm when the distance between neighbors is an important factor in the prediction.</p>]]></content:encoded>
            <category>ml</category>
            <category>knn</category>
            <category>weighted knn</category>
            <category>algo</category>
        </item>
        <item>
            <title><![CDATA[What is a Parametric Algorithm?]]></title>
            <link>https://stealthspectre.github.io/CyberSec-NGIT/blog/parametric</link>
            <guid>https://stealthspectre.github.io/CyberSec-NGIT/blog/parametric</guid>
            <pubDate>Wed, 29 Mar 2023 10:09:46 GMT</pubDate>
            <description><![CDATA[In machine learning, a parametric algorithm is a type of algorithm that makes assumptions about the underlying distribution of the data. These assumptions are typically based on a specific mathematical model, such as a linear regression model or a Gaussian distribution.]]></description>
            <content:encoded><![CDATA[<p>In machine learning, a parametric algorithm is a type of algorithm that makes assumptions about the underlying distribution of the data. These assumptions are typically based on a specific mathematical model, such as a linear regression model or a Gaussian distribution.</p><p>Parametric algorithms estimate the parameters of the model based on the training data, and then use these parameters to make predictions or decisions on new data. Because they make assumptions about the underlying distribution, parametric algorithms can be more efficient and require less data than non-parametric algorithms.</p><p>Examples of parametric algorithms include Linear <a href="/CyberSec-NGIT/blog/regression">Regression</a>, Logistic Regression, Naive Bayes, and Linear Discriminant Analysis (LDA). These algorithms are often used in classification and regression tasks, and can be effective in a wide range of applications. However, they may not be as flexible as non-parametric algorithms and may not perform well if the underlying assumptions are not met.</p><p>In parametric algorithms, we are assuming that the data follows a specific mathematical model or distribution. For example, in linear regression, we assume that the relationship between the input variables and the output variable is linear. In logistic regression, we assume that the output variable follows a logistic distribution. In Naive Bayes, we assume that the input variables are conditionally independent given the output variable. In Linear Discriminant Analysis (LDA), we assume that the input variables follow a multivariate normal distribution. These assumptions allow us to estimate the parameters of the model based on the training data, and then use these parameters to make predictions or decisions on new data. However, if the underlying assumptions are not met, the performance of the algorithm may be affected.</p>]]></content:encoded>
            <category>regression</category>
            <category>naive bayes</category>
            <category>lda</category>
            <category>guassian</category>
            <category>ml</category>
            <category>algo</category>
        </item>
        <item>
            <title><![CDATA[What is a Non Parametric Algorithm?]]></title>
            <link>https://stealthspectre.github.io/CyberSec-NGIT/blog/non-parametric</link>
            <guid>https://stealthspectre.github.io/CyberSec-NGIT/blog/non-parametric</guid>
            <pubDate>Wed, 29 Mar 2023 10:07:06 GMT</pubDate>
            <description><![CDATA[In machine learning, a non-parametric algorithm is a type of algorithm that does not make any assumptions about the underlying distribution of the data. This is in contrast to parametric algorithms, which assume that the data follows a specific distribution, such as a normal distribution.]]></description>
            <content:encoded><![CDATA[<p>In machine learning, a non-parametric algorithm is a type of algorithm that does not make any assumptions about the underlying distribution of the data. This is in contrast to parametric algorithms, which assume that the data follows a specific distribution, such as a normal distribution.</p><p>Non-parametric algorithms are often used when the underlying distribution of the data is unknown or cannot be easily modeled. Instead of making assumptions about the distribution, non-parametric algorithms rely on the data itself to make predictions or decisions. This makes them more flexible and adaptable to a wide range of data types and distributions.</p><p>Examples of non-parametric algorithms include <a href="/CyberSec-NGIT/blog/knn">K-Nearest Neighbors</a> (KNN), <a href="/CyberSec-NGIT/blog/decision-tree">Decision Trees</a>, Random Forests, and Support Vector Machines (SVMs). These algorithms are often used in classification and <a href="/CyberSec-NGIT/blog/regression">regression tasks</a>, and can be effective in a wide range of applications. However, they can also be computationally expensive and may require more data to achieve good performance compared to <a href="/CyberSec-NGIT/blog/parametric">parametric algorithms</a>.</p>]]></content:encoded>
            <category>random forest</category>
            <category>decision trees</category>
            <category>knn</category>
            <category>svm</category>
            <category>ml</category>
            <category>algo</category>
        </item>
        <item>
            <title><![CDATA[Decision Tree Classifier]]></title>
            <link>https://stealthspectre.github.io/CyberSec-NGIT/blog/decision-tree</link>
            <guid>https://stealthspectre.github.io/CyberSec-NGIT/blog/decision-tree</guid>
            <pubDate>Mon, 27 Mar 2023 18:58:24 GMT</pubDate>
            <description><![CDATA[- It is a type of Greedy Algorithm]]></description>
            <content:encoded><![CDATA[<ul><li>It is a type of <strong>Greedy Algorithm</strong></li><li>In this we try to prepare a model by taking a set of features and try to prepare a binary tree where at the end of leaf notes we get a part of one feature only</li><li>So basically we are classifying the given features with their conditions and dividing them by borders and seperating them</li><li>example let the set of data be this<ul><li>let our sample feature set be <img loading="lazy" src="https://i.imgur.com/X2jL7pJ.png" class="img_ev3q"></li><li>where red is one type of feature and green is other</li><li>where x axis denotes $X_0$ and y axis denotes $X_1$</li></ul></li><li>the nodes other than <strong>Leaf Nodes</strong> are called <strong>Decision Nodes</strong>, we can find the required optimal decision for each node using information theory</li><li><img loading="lazy" src="https://i.imgur.com/MCqpXVY.png" class="img_ev3q"><ul><li>this is our decision tree, and leaf nodes consists of the classification</li></ul></li><li>even if we want to add a new entity, we follow the decision nodes starting from top and like binary search tree if go through the tree and place it in its respective position</li><li>this is known as a greedy algorithm as we are finding the best case for our immediate sub task only, not considering future states prehandedly</li><li>using <strong>Information Theory</strong>, entropy and all we get optimal decision conditions (cause there can be many we should choose optimal condition) (it comes under <strong>Information Gain</strong>)</li><li>note: we use object oriented programming for implementing ml algorithms for ease of use and to use it more effectively</li><li>we use other quantifier for decision nodes along with <strong>Entropy</strong> which is known as <strong>Gini Index</strong></li></ul>]]></content:encoded>
            <category>ml</category>
            <category>classifier</category>
            <category>algo</category>
            <category>decision-tree</category>
        </item>
        <item>
            <title><![CDATA[ARP Scan]]></title>
            <link>https://stealthspectre.github.io/CyberSec-NGIT/blog/arp-scan</link>
            <guid>https://stealthspectre.github.io/CyberSec-NGIT/blog/arp-scan</guid>
            <pubDate>Mon, 27 Mar 2023 18:56:21 GMT</pubDate>
            <description><![CDATA[ARP]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="arp">ARP<a href="#arp" class="hash-link" aria-label="Direct link to ARP" title="Direct link to ARP">‚Äã</a></h2><ul><li>Address Resolution Protocol<ul><li>Basic ARP identifies MAC addresses and maps them to the IP addresses</li><li>the thing where it stores MAC address is known as ARP cache</li></ul></li><li>arp-scan is a command-line tool that uses the ARP protocol to discover and fingerprint IP hosts on the local network</li><li>arp-cache consists of associations our computer has learned about MAC addresses and IP addresses on the network<ul><li>initially we may get the one to the default gateway only</li></ul></li><li><code>arp -a</code> we get all the entries in the arp cache<ul><li>ex: <img loading="lazy" src="https://i.imgur.com/amp52LY.png" class="img_ev3q"></li><li>static type means it has been feeded statically whereas dynamic type means, it had to learn about it</li><li>physical address is the mapped MAC addresses</li></ul></li><li>we can delete a specific entry by <code>arp -d &lt;entry ip&gt;</code></li><li>but what if we want to send arp requests to an external web server<ul><li>we send it through the default gateway, that is in our case is the router</li><li>then router sends its mac address to the requester and then the requester sends the external web IP address to router and the router handles the rest</li></ul></li><li><code>sudo arp-scan -l</code> we get all the information about hosts in the network (but with arp cache, we may not still learn all the new info about computer)</li><li>but <code>-l</code> is very noisy and can be easily detectable</li><li>tools like netdiscover are used for stealthy scans</li><li>[<!-- -->[ARP Poisoning]<!-- -->] is a type of MITM where hacker utilizes these ARP requests to steal info</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="arp-poisoning">ARP Poisoning<a href="#arp-poisoning" class="hash-link" aria-label="Direct link to ARP Poisoning" title="Direct link to ARP Poisoning">‚Äã</a></h2><ul><li>let this be our initial system <img loading="lazy" src="https://i.imgur.com/koBbVfx.png" class="img_ev3q"></li><li>then B turned out to be a hacker <img loading="lazy" src="https://i.imgur.com/kaPi0rs.png" class="img_ev3q"></li><li>observe A's intial ARP cache for default gateway which points to the router</li><li>now the hacker sends specific ARP requests to A where he changes the default gateway of A to his address, so now if A wants to communicate with the router, it sends requests to its IP in which it follows the MAC address from ARP Cache, but our hacker B has changed that IP address mapping of router in the ARP cache to his address, so all the requests will be redirected to him and first he grabs the information and then sends to the router</li><li>This is called Man In The Middle Attack <img loading="lazy" src="https://i.imgur.com/V6G5qfr.png" class="img_ev3q"></li><li>We again do in such a way where we want the router to send result back to us instead of A first</li><li>In Kali we use Ettercap to perform this attack</li><li><a href="https://www.youtube.com/watch?v=A7nih6SANYs" target="_blank" rel="noopener noreferrer">Procedure video</a> From 3:38</li><li>Using [<!-- -->[DNS Cache Poisoning]<!-- -->] you can make this attack more better</li></ul>]]></content:encoded>
            <category>arp</category>
            <category>arp-poisoning</category>
            <category>man-in-the-middle</category>
        </item>
    </channel>
</rss>